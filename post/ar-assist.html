<!DOCTYPE html>
<html>
<head>
    <title>Han - FM-AR Assistance</title>

    <!-- meta -->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- css -->
    <link rel="stylesheet" href="../css/bootstrap.min.css">
    <link rel="stylesheet" href="../css/ionicons.min.css">
    <link rel="stylesheet" href="../css/pace.css">
    <link rel="stylesheet" href="../css/custom.css">

    <!-- js -->
    <script src="../js/jquery-2.1.3.min.js"></script>
    <script src="../js/bootstrap.min.js"></script>
    <script src="../js/pace.min.js"></script>
    <script src="../js/modernizr.custom.js"></script>
    <script src="../js/script.js"></script>
    <script src="../components/loader.js"></script>
</head>

<body id="single">
<!-- Header Container - Content loaded by loader.js -->
<div id="header-container" class="container"></div>

<div class="content-body">
    <div class="container">
        <div class="row">
            <main class="col-md-8">
                <article class="post post-1">
                    <header class="entry-header">
                        <h1 class="entry-title">Your Tesla can see more than you do? Not anymore.</h1>
                    </header>
                    <div class="entry-meta">
                        <a>V2X-based Driving Perception Assistance System</a>
                    </div>
                    <figcaption style="font-size: 12px">Application demo in simulated VR environment: Traffic Light Recognition for visual impaired driver</figcaption>
                    <div class="entry-content clearfix">
                        <div style="display: flex; justify-content: space-between;">
                            <figure>
                                <img src="../img/ar-assit-demo-0.png" alt="Normal View Traffic Light" class="img-responsive" style="width: 400px; height: 300px;">
                                <figcaption style="font-size: 12px">Normal view of traffic light.</figcaption>
                            </figure>
                            <figure>
                                <img src="../img/ar-assit-demo-1.png" alt="Color Blind Mode Traffic Light" class="img-responsive" style="width: 400px; height: 300px;">
                                <figcaption style="font-size: 12px">Traffic light in protanopia mode.</figcaption>
                            </figure>
                            <figure>
                                <img src="../img/ar-assit-demo-2.gif" alt="Traffic Light Phase Indicator" class="img-responsive" style="width: 400px; height: 300px;">
                                <figcaption style="font-size: 12px">Traffic light phase indicator with our system.</figcaption>
                            </figure>
                        </div>

                        <p>Our research focuses on integrating foundation models (FMs) with augmented reality (AR) to develop a more interactive and responsive driving assistance system. This project aims to leverage the multimodal perception and generation capabilities of FMs to interpret complex driver commands and provide real-time visual aids, such as traffic object identification and highlighting.</p>
                        <p>The system employ advanced data collection techniques using onboard sensors like Lidar and cameras, along with external sources such as high-definition maps and real-time traffic updates. This data will be processed through a multi-modal data fusion algorithm to create a cohesive understanding of the environment, enabling accurate 3D reconstruction and amodal tracking of objects.</p>
                        <figure>
                            <img src="../img/ar-assit-diag.png" alt="System Pipeline" class="img-responsive">
                            <figcaption style="font-size: 12px">System pipeline from data collection to AR visualization.</figcaption>
                        </figure>
                        <p>A key innovation in our approach is the use of a structured Scenegraph, proposed in <a href="gromit.html">previous work (GROMIT)</a>, to translate the complexity of the 3D world into a format understandable by FMs. This allows the system to process varied natural language commands and generate executable scripts for AR rendering tasks. A specialized large language model (LLM), trained on driving-related tasks, will interpret the Scenegraph and natural language queries to produce relevant scripts.</p>
                        <p>The system's design emphasizes adaptability through human-in-loop interactions, where drivers provide feedback in natural language. This iterative learning process allows the FM to refine its parameters and enhance its specialization in specific functionalities, improving the system's effectiveness over time.</p>

                        <figure>
                            <img src="../img/ar-assit-amodal-cam.png" alt="Amodal Tracking in Unity" class="img-responsive">
                            <figcaption style="font-size: 12px">Amodal tracking demo in Unity simulated environment.</figcaption>
                        </figure>

                    </div>
                </article>
            </main>
            <aside class="col-md-4">
                <div class="widget widget-recent-posts">
                    <h3 class="widget-title">Related Links</h3>
                    <ul>
                        <li>
                            <a href="https://www.youtube.com/watch?v=Riky0i48I5E">Demo Video in VR</a>
                        </li>
                        <li>
                            <a href="./gromit.html">GROMIT: Runtime Behavior Generation in Unity</a>
                        </li>
                    </ul>
                </div>
            </aside>
        </div>
    </div>
</div>

<div class="read-more cl-effect-14">
    <a href="../index.html" class="more-link"><span class="meta-nav">‚Üê</span> Back</a>
</div>

<!-- Footer Container - Content loaded by loader.js -->
<div class="height-40px"></div>
<div id="footer-container"></div>
</body>
</html>
